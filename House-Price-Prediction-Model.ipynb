{"cells":[{"metadata":{"_uuid":"17c88af1-2059-4831-9340-6ae491149823","_cell_guid":"f3f0a807-6599-4388-aad5-81cfd8b613ab","trusted":true},"cell_type":"code","source":"# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code]\nimport seaborn as sns\nimport warnings \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math\n\n# %% [code]\ndf_train = pd.read_csv(r'../input/train-house/Train.csv')\ndf_test = pd.read_csv(r'../input/test-house/Test.csv')\n\n# %% [code]\n## Select object data type columns. \nobject_columns_train = df_train.select_dtypes(include=['object'])\n\n## Select numeric data type columns.\nnumerical_columns_train =df_train.select_dtypes(exclude=['object'])\n\n# %% [code]\n## Fill missing values in society column with 'None'.\nobject_columns_train['society'] = object_columns_train['society'].fillna('None')\n\n# %% [code]\n## Fill missing values in size & location column with most frequent value.\n\ncols = ['size', 'location']\nobject_columns_train[cols] = object_columns_train[cols].fillna(object_columns_train.mode().iloc[0])\n\n# %% [code]\n## FIll missing values in bath column with its median.\n\nmedian = numerical_columns_train['bath'].median()\nnumerical_columns_train['bath'] = numerical_columns_train['bath'].fillna(median)\n\n# %% [code]\n## Fill missing values in balcony column with 0\n\nnumerical_columns_train['balcony'] = numerical_columns_train['balcony'].fillna(0)\n\n# %% [code]\n## Remove availability column from dataset due to low variance.\n\nobject_columns_train.drop(['availability'], axis=1, inplace=True)\n\n# %% [code]\n#Using One hot encoder on categorical variables \n\nobject_columns_train = pd.get_dummies(object_columns_train, columns= object_columns_train.columns) \n\n# %% [code]\nobject_columns_train.head(3)\n\n# %% [code]\n## Concat Categorical (after encoding) and numerical features\n\ndf_final_train = pd.concat([object_columns_train, numerical_columns_train], axis=1,sort=False)\ndf_final_train.head()\n\n# %% [code]\ndf_final_train.shape\n\n# %% [code]\n## Make copy of train & test dataset.\ndf_train_copy = df_train.copy()\ndf_test_copy = df_test.copy()\n\n# %% [markdown]\n# ### Prepare test dataset \n\n# %% [code]\n## Drop price column which has to be predicted.\n\ndf_test_copy.drop(['price'], axis=1, inplace=True)\n\n# %% [markdown]\n# ## XGB Regressor\n\n# %% [code]\n##Separate Train and Targets\n\ntarget= np.log(df_final_train['price'])\n\ndf_final_train.drop(['price'],axis=1, inplace=True)\n\n# %% [code]\ndf_final_train.shape\n\n# %% [code]\nx_train,x_test,y_train,y_test = train_test_split(df_final_train, target, test_size=0.3,random_state=15)\n\n# %% [code]\nxgb =XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2000,\n             n_jobs=1, nthread=None, objective='reg:linear',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1, tree_method='gpu_hist')\n\n\n# %% [code]\n#Fitting\nxgb.fit(x_train, y_train)\n\n# %% [markdown]\n# ## LGB Regressor\n\n# %% [code]\ndf_train_lgb = df_final_train.copy()\ntarget_lgb = target.copy()\n\n# %% [code]\nimport re\ndf_train_lgb = df_train_lgb.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\n# %% [code]\nx_train1,x_test1,y_train1,y_test1 = train_test_split(df_train_lgb, target_lgb, test_size=0.3,random_state=15)\n\n# %% [code]\nlgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=10000,\n                                       max_bin=180,\n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.4, \n                                       device= 'gpu',\n                                       gpu_platform_id= 0,\n                                       gpu_device_id=0\n                                       )\n\n# %% [code]\n#Fitting\nlgbm.fit(x_train1, y_train1, eval_metric='rmse')\n\n# %% [markdown]\n# ### Prediction from XGB Regressor and LGB Regressor both.\n\n# %% [code]\npredict1 = xgb.predict(x_test)\npredict = lgbm.predict(x_test)\n\n# %% [code]\n## Error check\nprint('Root Mean Square Error test = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict1))))\nprint('Root Mean Square Error test = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict))))\n\n# %% [markdown]\n# ### Fitting and predicting whole train dataset from XGB & LGB Regressor.\n\n# %% [code]\n## Fit with all dataset in XGB Regressor\nxgb.fit(df_final_train, target)\n\n# %% [code]\nlgbm.fit(df_train_lgb, target_lgb, eval_metric='rmse')\n\n# %% [markdown]\n# ### Now, predict test dataset. \n\n# %% [code]\n#predict4 = lgbm.predict(df_test_copy)\npredict3 = xgb.predict(df_test_copy)\n\n# %% [code]\n","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}